# -*- coding: utf-8 -*-
"""intro to A/b Testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zNDQQWlcVQhlaeSk4-rhFjDy3Y-0Mxdh

# Understanding two tailed test

**consider a problem,**

Amazon is testing a new delivery system to cut down delivery time and wants to see if it should be implemented company wide. Current mean delivery time is 2 days.

# **Null >= 2** (that's what the current delivery system is given us)
# **Alternative hypothesis < 2** (we gonna know if the current delivery system is gonna cut it down to less than 2 days)

# How to implement this system?

![](https://www.researchgate.net/publication/320285208/figure/fig2/AS:547713716584449@1507596534948/P-value-on-the-left-and-right-tailed-graphic-with-null-hypothesis.png)

let's say amazon will take sample of 1000 orders and see if that delivery time been cut down( <2),if yes then we have evidence to point  to alternative hypothesis and we are gonna implement our new delivery system,otherwise we won't

![](https://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg)

# 2 tailed test

![](https://www.fromthegenesis.com/wp-content/uploads/2018/06/Types-of-Hypothesis-Tests.jpg)
"""





"""# Choosing variables

1. A control group - They'll be shown the old design

2. A treatment (or experimental) group - They'll be shown the new design

systematic difference between the groups is the design of the product page

# Choose a sample size

**Power analysis** is normally conducted before the data collection.  The main purpose underlying power analysis is to help the researcher to determine the smallest sample size that is suitable to detect the effect of a given test at the desired level of significance.

The power of a binary hypothesis test is the probability that the test rejects the null hypothesis when a specific alternative hypothesis is true — i.e., it indicates the probability of avoiding a type II error.
"""

# Commented out IPython magic to ensure Python compatibility.
# Packages imports
import numpy as np
import pandas as pd
import scipy.stats as stats
import statsmodels.stats.api as sms
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from math import ceil

# %matplotlib inline

# Some plot styling preferences
plt.style.use('seaborn-whitegrid')
font = {'family' : 'Helvetica',
        'weight' : 'bold',
        'size'   : 14}

mpl.rc('font', **font)

'''
* Effect size is a quantitative measure of the magnitude of the experimental effect. 
The larger the effect size the stronger the relationship between two variables. 
You can look at the effect size when comparing any two groups to see how substantially different they are.


* How big of a difference we expect there to be between the conversion rates
'''
effect_size = sms.proportion_effectsize(0.13, 0.15)    # Calculating effect size based on our expected rates

print(effect_size)
'''
power=0.8 == 80% chance to detect it as statistically significant in our test with the sample size we 
calculate.
'''

#https://www.statsmodels.org/stable/generated/statsmodels.stats.power.NormalIndPower.solve_power.html

required_n = sms.NormalIndPower().solve_power(
    effect_size, 
    power=0.8, 
    alpha=0.05, 
    ratio=1
    )                                                  # Calculating sample size needed

required_n = ceil(required_n)                          # Rounding up to next whole number                          

print(required_n)

#We’d need at least 4720 observations for each group.



df = pd.read_csv('/content/ab_data.csv')

df.head()

df.info()

pd.crosstab(df['group'], df['landing_page'])

session_counts  = df['user_id'].value_counts(ascending=False)
multi_users = session_counts[session_counts > 1].count()

print(f'There are {multi_users} users that appear multiple times in the dataset')

users_to_drop = session_counts[session_counts > 1].index

df = df[~df['user_id'].isin(users_to_drop)]
print(f'The updated dataset now has {df.shape[0]} entries')

control_sample = df[df['group'] == 'control'].sample(n=required_n, random_state=22)
treatment_sample = df[df['group'] == 'treatment'].sample(n=required_n, random_state=22)

ab_test = pd.concat([control_sample, treatment_sample], axis=0)
ab_test.reset_index(drop=True, inplace=True)
ab_test

ab_test.info()

ab_test['group'].value_counts()

conversion_rates = ab_test.groupby('group')['converted']

std_p = lambda x: np.std(x, ddof=0)              # Std. deviation of the proportion
se_p = lambda x: stats.sem(x, ddof=0)            # Std. error of the proportion (std / sqrt(n))

conversion_rates = conversion_rates.agg([np.mean, std_p, se_p])
conversion_rates.columns = ['conversion_rate', 'std_deviation', 'std_error']


conversion_rates.style.format('{:.3f}')

plt.figure(figsize=(8,6))

sns.barplot(x=ab_test['group'], y=ab_test['converted'], ci=False)

plt.ylim(0, 0.17)
plt.title('Conversion rate by group', pad=20)
plt.xlabel('Group', labelpad=15)
plt.ylabel('Converted (proportion)', labelpad=15);

from statsmodels.stats.proportion import proportions_ztest, proportion_confint
control_results = ab_test[ab_test['group'] == 'control']['converted']
treatment_results = ab_test[ab_test['group'] == 'treatment']['converted']
n_con = control_results.count()
n_treat = treatment_results.count()
successes = [control_results.sum(), treatment_results.sum()]
nobs = [n_con, n_treat]

z_stat, pval = proportions_ztest(successes, nobs=nobs)
(lower_con, lower_treat), (upper_con, upper_treat) = proportion_confint(successes, nobs=nobs, alpha=0.05)

print(f'z statistic: {z_stat:.2f}')
print(f'p-value: {pval:.3f}')
print(f'ci 95% for control group: [{lower_con:.3f}, {upper_con:.3f}]')
print(f'ci 95% for treatment group: [{lower_treat:.3f}, {upper_treat:.3f}]')

#statistically insignificant as p value is higher than 0.05 so we will stick with our old design