{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BanglaSentiment w/o translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-zZRwOic_CV",
        "outputId": "59675a2a-4ae5-482c-98d9-41f5d9770cfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQnxO_h-c_jV",
        "outputId": "a2005540-cf2d-4af1-ce16-306e169d2186",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install git+https://github.com/BoseCorp/py-googletrans.git --upgrade\n",
        "!pip install vaderSentiment\n",
        "!pip install transformers\n",
        "\n",
        "!pip3 install pycld3\n",
        "!pip3 install regex\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from googletrans import Translator\n",
        "from transformers import pipeline\n",
        "import numpy as np \n",
        "import IPython\n",
        "import operator\n",
        "import re\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/BoseCorp/py-googletrans.git\n",
            "  Cloning https://github.com/BoseCorp/py-googletrans.git to /tmp/pip-req-build-ahetwzdt\n",
            "  Running command git clone -q https://github.com/BoseCorp/py-googletrans.git /tmp/pip-req-build-ahetwzdt\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from googletrans==2.3.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans==2.3.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans==2.3.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans==2.3.0) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans==2.3.0) (3.0.4)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-2.3.0-cp36-none-any.whl size=16447 sha256=8ef378f4320d307b986c49d47a449df754b2c66ec58523525e1aed30d7ad3ebb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-negur4hu/wheels/6a/fc/9e/2d31d95d9e97da5166afd8225a6f3b6850dc2c6e84accefbfc\n",
            "Successfully built googletrans\n",
            "Installing collected packages: googletrans\n",
            "Successfully installed googletrans-2.3.0\n",
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.6.20)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=aaf4bba160bfd985eeefffb7aaa75bde1147624b7fb0d3ee510adc50281d3368\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bayF2au_wu17"
      },
      "source": [
        "# installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGGvkymfOh-k"
      },
      "source": [
        "\n",
        "\n",
        "#taken from : https://github.com/sagorbrur/bendeep/blob/master/bendeep/sentiment.py\n",
        "\n",
        "# from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import json\n",
        "\n",
        "\n",
        "# Collected from : https://github.com/sagorbrur/bendeep/tree/master/models/sentiment\n",
        "\n",
        "model_path = \"./gdrive/My Drive/banglaSentiment/sentiment/senti_trained.pt\"\n",
        "vocab_path = \"./gdrive/My Drive/banglaSentiment/sentiment/vocab.txt\"\n",
        "\n",
        "def save_dict_to_file(dic):\n",
        "    f = open('vocab.txt','w')\n",
        "    f.write(str(dic))\n",
        "    f.close()\n",
        "\n",
        "\n",
        "class Sequences_train(Dataset):\n",
        "    def __init__(self, path, max_seq_len):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        df = pd.read_csv(path)\n",
        "        vectorizer = CountVectorizer(min_df=0.015)\n",
        "        vectorizer.fit(df.review.tolist())\n",
        "\n",
        "        save_dict_to_file(vectorizer.vocabulary_)\n",
        "        self.token2idx = vectorizer.vocabulary_\n",
        "        self.token2idx['<PAD>'] = max(self.token2idx.values()) + 1\n",
        "\n",
        "        tokenizer = vectorizer.build_analyzer()\n",
        "        self.encode = lambda x: [self.token2idx[token] for token in tokenizer(x)\n",
        "                                 if token in self.token2idx]\n",
        "        self.pad = lambda x: x + (max_seq_len - len(x)) * [self.token2idx['<PAD>']]\n",
        "        \n",
        "        sequences = [self.encode(sequence)[:max_seq_len] for sequence in df.review.tolist()]\n",
        "        sequences, self.labels = zip(*[(sequence, label) for sequence, label\n",
        "                                    in zip(sequences, df.sentiment.tolist()) if sequence])\n",
        "        self.sequences = [self.pad(sequence) for sequence in sequences]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        assert len(self.sequences[i]) == self.max_seq_len\n",
        "        return self.sequences[i], self.labels[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "\n",
        "class Sequences_infer(Dataset):\n",
        "    def __init__(self, vocab_path, max_seq_len):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        vectorizer = CountVectorizer(min_df=0.015)\n",
        "        \n",
        "        vocab = open(vocab_path, 'r').read()\n",
        "        vocab = vocab.replace(\"'\", \"\\\"\")\n",
        "        vocab = json.loads(vocab)\n",
        "        # print(vocab)\n",
        "        self.token2idx = vocab\n",
        "        # print(self.token2idx)\n",
        "        self.token2idx['<PAD>'] = max(self.token2idx.values()) + 1\n",
        "        # print(self.token2idx)\n",
        "\n",
        "        tokenizer = vectorizer.build_analyzer()\n",
        "        self.encode = lambda x: [self.token2idx[token] for token in tokenizer(x)\n",
        "                                 if token in self.token2idx]\n",
        "        self.pad = lambda x: x + (max_seq_len - len(x)) * [self.token2idx['<PAD>']]\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        batch_size,\n",
        "        embedding_dimension=100,\n",
        "        hidden_size=128, \n",
        "        n_layers=1,\n",
        "        device='cpu',\n",
        "    ):\n",
        "        super(RNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.encoder = nn.Embedding(vocab_size, embedding_dimension)\n",
        "        self.rnn = nn.GRU(\n",
        "            embedding_dimension,\n",
        "            hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.decoder = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return torch.randn(self.n_layers, self.batch_size, self.hidden_size).to(self.device)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        # Avoid breaking if the last batch has a different size\n",
        "        batch_size = inputs.size(0)\n",
        "        if batch_size != self.batch_size:\n",
        "            self.batch_size = batch_size\n",
        "            \n",
        "        encoded = self.encoder(inputs)\n",
        "        output, hidden = self.rnn(encoded, self.init_hidden())\n",
        "        output = self.decoder(output[:, :, -1]).squeeze()\n",
        "        return output\n",
        "\n",
        "def collate(batch):\n",
        "    inputs = torch.LongTensor([item[0] for item in batch])\n",
        "    target = torch.FloatTensor([item[1] for item in batch])\n",
        "    return inputs, target\n",
        "\n",
        "def train(data_path, batch_size = 64, epochs=100, model_name=\"trained.pt\"):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  dataset = Sequences_train(data_path, max_seq_len=128)\n",
        "  train_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate)\n",
        "  model = RNN(\n",
        "    hidden_size=128,\n",
        "    vocab_size=len(dataset.token2idx),\n",
        "    device=device,\n",
        "    batch_size=batch_size,\n",
        "  )\n",
        "  model = model.to(device)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)\n",
        "  model.train()\n",
        "  train_losses = []\n",
        "  for epoch in range(epochs):\n",
        "      progress_bar = tqdm_notebook(train_loader, leave=False)\n",
        "      losses = []\n",
        "      total = 0\n",
        "      for inputs, target in progress_bar:\n",
        "          inputs, target = inputs.to(device), target.to(device\n",
        "                                                      )\n",
        "          model.zero_grad()\n",
        "          \n",
        "          output = model(inputs)\n",
        "      \n",
        "          loss = criterion(output, target)\n",
        "          \n",
        "          loss.backward()\n",
        "                \n",
        "          nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
        "\n",
        "          optimizer.step()\n",
        "          \n",
        "          progress_bar.set_description(f'Loss: {loss.item():.3f}')\n",
        "          \n",
        "          losses.append(loss.item())\n",
        "          total += 1\n",
        "      \n",
        "      epoch_loss = sum(losses) / total\n",
        "      train_losses.append(epoch_loss)\n",
        "\n",
        "      tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')\n",
        "\n",
        "  # torch.save(model, model_name)\n",
        "  torch.save(model.state_dict(), model_name)\n",
        "\n",
        "\n",
        "def analyze(model_path, vocab_path, text, batch_size=64):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    dataset = Sequences_infer(vocab_path, max_seq_len=128)\n",
        "    # model = torch.load(model_path)\n",
        "    model = RNN(\n",
        "    hidden_size=128,\n",
        "    vocab_size=len(dataset.token2idx),\n",
        "    device=device,\n",
        "    batch_size=batch_size,\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    sum=0\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    for txt in range(len(text)):\n",
        "      with torch.no_grad():\n",
        "        try:\n",
        "          test_vector = torch.LongTensor([dataset.pad(dataset.encode(str(text[txt])))]).to(device)\n",
        "  \n",
        "        except:\n",
        "          print(\"Long sentence?\\n\")\n",
        "          continue\n",
        "        \n",
        "        output = model(test_vector)\n",
        "        prediction = torch.sigmoid(output).item()\n",
        "\n",
        "        if prediction > 0.5:\n",
        "          sum+=1\n",
        "          pos+=1\n",
        "        else:\n",
        "          sum-=1\n",
        "          neg+=1\n",
        "    print(\"total sentences = \",len(text))\n",
        "    print(\"total positive sentences = \",pos)\n",
        "    print(\"total negative sentences = \",neg)\n",
        "\n",
        "    return sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pyZiKeFmekw",
        "outputId": "d8db9c44-3c3d-4559-e083-743acc86d841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "#https://github.com/Markopolo-ai/DatasetCollection/blob/master/data/prothom-alo.csv\n",
        "df=pd.read_csv('./gdrive/My Drive/banglaSentiment/prothom-alo.csv')\n",
        "\n",
        "for i in range(len(df)):\n",
        "  print(\"\\n\\n-> Making Prediction for Document = \",i+1)\n",
        "  parsed_text = df.Text[i].split('।')\n",
        "  '''\n",
        "  for rnglish,we can do : parsed_text = df.Text[i].split('.')\n",
        "  and then send parsed_text to textblob,vader or other models that understand english well for sentiment\n",
        "  '''\n",
        "  sum = analyze(model_path, vocab_path, parsed_text)\n",
        "  if(sum >= 0):\n",
        "    print(\"-------------------------Positive Document-------------------------\")\n",
        "  else:\n",
        "    print(\"-------------------------Negative Document-------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "-> Making Prediction for Document =  1\n",
            "total sentences =  23\n",
            "total positive sentences =  9\n",
            "total negative sentences =  14\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  2\n",
            "total sentences =  36\n",
            "total positive sentences =  11\n",
            "total negative sentences =  25\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  3\n",
            "total sentences =  34\n",
            "total positive sentences =  13\n",
            "total negative sentences =  21\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  4\n",
            "total sentences =  23\n",
            "total positive sentences =  10\n",
            "total negative sentences =  13\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  5\n",
            "total sentences =  33\n",
            "total positive sentences =  7\n",
            "total negative sentences =  26\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  6\n",
            "total sentences =  42\n",
            "total positive sentences =  9\n",
            "total negative sentences =  33\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  7\n",
            "total sentences =  10\n",
            "total positive sentences =  4\n",
            "total negative sentences =  6\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  8\n",
            "total sentences =  56\n",
            "total positive sentences =  22\n",
            "total negative sentences =  34\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  9\n",
            "total sentences =  19\n",
            "total positive sentences =  8\n",
            "total negative sentences =  11\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  10\n",
            "total sentences =  27\n",
            "total positive sentences =  4\n",
            "total negative sentences =  23\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  11\n",
            "total sentences =  39\n",
            "total positive sentences =  11\n",
            "total negative sentences =  28\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  12\n",
            "total sentences =  17\n",
            "total positive sentences =  4\n",
            "total negative sentences =  13\n",
            "-------------------------Negative Document-------------------------\n",
            "\n",
            "\n",
            "-> Making Prediction for Document =  13\n",
            "total sentences =  42\n",
            "total positive sentences =  13\n",
            "total negative sentences =  29\n",
            "-------------------------Negative Document-------------------------\n",
            "CPU times: user 3.2 s, sys: 830 ms, total: 4.03 s\n",
            "Wall time: 4.06 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAe9RDkvwWs9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}